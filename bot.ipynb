{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPnYC0alB698R+XxpXCLcx9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MarriRohan/Stock-Trading-Bot-with-Deep-Q-Learning/blob/main/bot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Original cell PW7NFh8Ojz5g is now empty as its content has been moved to new cells.\n",
        "# Please execute the newly created cells in order (env, train_dqn, live_trader, risk).\n",
        "# Also ensure 'alerts' and 'broker' placeholder cells are executed if they exist."
      ],
      "metadata": {
        "id": "PW7NFh8Ojz5g"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6f32f5f",
        "outputId": "883b8248-cf0d-4b0f-dc59-1ecd30f8d426"
      },
      "source": [
        "# Create an instance of the SLTPEnv using the dummy data\n",
        "# Assumes price_array, features, and regime_ids are already defined from new_cell_data\n",
        "# Assumes SLTPEnv class is defined from new_cell_env\n",
        "\n",
        "env = SLTPEnv(price_array, features, regime_ids, max_capital=50000)\n",
        "print(\"SLTPEnv instance created successfully.\")"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SLTPEnv instance created successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fc7981e",
        "outputId": "0cc200d4-5ed3-4940-a746-c7bc0faede32"
      },
      "source": [
        "import time\n",
        "import random\n",
        "\n",
        "print(\"Starting dynamic live trading simulation...\")\n",
        "\n",
        "simulation_ticks = 20 # Number of simulated market ticks\n",
        "\n",
        "for i in range(simulation_ticks):\n",
        "    print(f\"\\n--- Simulating Tick {i+1}/{simulation_ticks} ---\")\n",
        "    # Create a new dummy tick to simulate market data arrival\n",
        "    current_tick = DummyTick()\n",
        "    current_tick.price = 100.0 + random.uniform(-0.5, 0.5) # Simulate price fluctuation\n",
        "    current_tick.features = np.random.randn(num_features_from_x).astype(np.float32)\n",
        "    current_tick.features[-1] = np.abs(current_tick.features[-1]) * 0.5 + 0.1 # Ensure ATR is positive\n",
        "    current_tick.regime_id = np.random.randint(0, num_regimes)\n",
        "\n",
        "    # Call the on_market_tick function from the live_trader\n",
        "    on_market_tick(current_tick)\n",
        "\n",
        "    # Simulate some time passing\n",
        "    time.sleep(0.5) # Pause for 0.5 seconds to visualize the dynamic updates\n",
        "\n",
        "print(\"Dynamic live trading simulation complete.\")\n"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting dynamic live trading simulation...\n",
            "\n",
            "--- Simulating Tick 1/20 ---\n",
            "Placing BUY order for 704 of DUMMY with SL=99.18739886714408, TP=100.25151504708717, Trailing=0.3547053933143616\n",
            "ALERT: Placed order ORDER_DUMMY_704 qty=704 SL=99.19 TP=100.25 trail=0.35\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-708000993.py:83: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  action = int(model.predict(live_state.observation.reshape(1, -1), deterministic=True)[0])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Simulating Tick 2/20 ---\n",
            "Placing BUY order for 1694 of DUMMY with SL=99.92441639367414, TP=100.36707788769078, Trailing=0.14755383133888245\n",
            "ALERT: Placed order ORDER_DUMMY_1694 qty=1694 SL=99.92 TP=100.37 trail=0.15\n",
            "\n",
            "--- Simulating Tick 3/20 ---\n",
            "Placing BUY order for 554 of DUMMY with SL=99.51887692408012, TP=100.87162355975555, Trailing=0.45091554522514343\n",
            "ALERT: Placed order ORDER_DUMMY_554 qty=554 SL=99.52 TP=100.87 trail=0.45\n",
            "\n",
            "--- Simulating Tick 4/20 ---\n",
            "Placing BUY order for 869 of DUMMY with SL=100.18482306861694, TP=101.04739439749534, Trailing=0.2875237762928009\n",
            "ALERT: Placed order ORDER_DUMMY_869 qty=869 SL=100.18 TP=101.05 trail=0.29\n",
            "\n",
            "--- Simulating Tick 5/20 ---\n",
            "Placing BUY order for 2332 of DUMMY with SL=99.53967146228175, TP=99.861244809135, Trailing=0.10719111561775208\n",
            "ALERT: Placed order ORDER_DUMMY_2332 qty=2332 SL=99.54 TP=99.86 trail=0.11\n",
            "\n",
            "--- Simulating Tick 6/20 ---\n",
            "Placing BUY order for 624 of DUMMY with SL=99.45997408695548, TP=100.6617914802584, Trailing=0.40060579776763916\n",
            "ALERT: Placed order ORDER_DUMMY_624 qty=624 SL=99.46 TP=100.66 trail=0.40\n",
            "\n",
            "--- Simulating Tick 7/20 ---\n",
            "Placing BUY order for 2272 of DUMMY with SL=99.45196605048928, TP=99.78200715355429, Trailing=0.1100137010216713\n",
            "ALERT: Placed order ORDER_DUMMY_2272 qty=2272 SL=99.45 TP=99.78 trail=0.11\n",
            "\n",
            "--- Simulating Tick 8/20 ---\n",
            "Placing BUY order for 1808 of DUMMY with SL=99.99357102059656, TP=100.40835220360094, Trailing=0.1382603943347931\n",
            "ALERT: Placed order ORDER_DUMMY_1808 qty=1808 SL=99.99 TP=100.41 trail=0.14\n",
            "\n",
            "--- Simulating Tick 9/20 ---\n",
            "Placing BUY order for 1125 of DUMMY with SL=100.21727203467584, TP=100.88393803575731, Trailing=0.2222220003604889\n",
            "ALERT: Placed order ORDER_DUMMY_1125 qty=1125 SL=100.22 TP=100.88 trail=0.22\n",
            "\n",
            "--- Simulating Tick 10/20 ---\n",
            "Placing BUY order for 2881 of DUMMY with SL=100.32349067325212, TP=100.5837765722952, Trailing=0.0867619663476944\n",
            "ALERT: Placed order ORDER_DUMMY_2881 qty=2881 SL=100.32 TP=100.58 trail=0.09\n",
            "\n",
            "--- Simulating Tick 11/20 ---\n",
            "Placing BUY order for 682 of DUMMY with SL=99.31596348775513, TP=100.41428114069588, Trailing=0.3661058843135834\n",
            "ALERT: Placed order ORDER_DUMMY_682 qty=682 SL=99.32 TP=100.41 trail=0.37\n",
            "\n",
            "--- Simulating Tick 12/20 ---\n",
            "Placing BUY order for 1707 of DUMMY with SL=99.91430801382934, TP=100.35358880690013, Trailing=0.14642693102359772\n",
            "ALERT: Placed order ORDER_DUMMY_1707 qty=1707 SL=99.91 TP=100.35 trail=0.15\n",
            "\n",
            "--- Simulating Tick 13/20 ---\n",
            "Placing BUY order for 633 of DUMMY with SL=99.69107645744117, TP=100.87590268248351, Trailing=0.3949420750141144\n",
            "ALERT: Placed order ORDER_DUMMY_633 qty=633 SL=99.69 TP=100.88 trail=0.39\n",
            "\n",
            "--- Simulating Tick 14/20 ---\n",
            "Placing BUY order for 444 of DUMMY with SL=99.27518840185932, TP=100.9610379492455, Trailing=0.5619498491287231\n",
            "ALERT: Placed order ORDER_DUMMY_444 qty=444 SL=99.28 TP=100.96 trail=0.56\n",
            "\n",
            "--- Simulating Tick 15/20 ---\n",
            "Placing BUY order for 4037 of DUMMY with SL=99.48881090965523, TP=99.67455469977511, Trailing=0.0619145967066288\n",
            "ALERT: Placed order ORDER_DUMMY_4037 qty=4037 SL=99.49 TP=99.67 trail=0.06\n",
            "\n",
            "--- Simulating Tick 16/20 ---\n",
            "Placing BUY order for 659 of DUMMY with SL=99.53926609721803, TP=100.67664592233324, Trailing=0.3791266083717346\n",
            "ALERT: Placed order ORDER_DUMMY_659 qty=659 SL=99.54 TP=100.68 trail=0.38\n",
            "\n",
            "--- Simulating Tick 17/20 ---\n",
            "Placing BUY order for 610 of DUMMY with SL=99.94081302874699, TP=101.16980155699864, Trailing=0.4096628427505493\n",
            "ALERT: Placed order ORDER_DUMMY_610 qty=610 SL=99.94 TP=101.17 trail=0.41\n",
            "\n",
            "--- Simulating Tick 18/20 ---\n",
            "Placing BUY order for 827 of DUMMY with SL=99.53260324400688, TP=100.43858675164009, Trailing=0.3019945025444031\n",
            "ALERT: Placed order ORDER_DUMMY_827 qty=827 SL=99.53 TP=100.44 trail=0.30\n",
            "\n",
            "--- Simulating Tick 19/20 ---\n",
            "Placing BUY order for 1701 of DUMMY with SL=100.01329914163176, TP=100.45418617557112, Trailing=0.14696234464645386\n",
            "ALERT: Placed order ORDER_DUMMY_1701 qty=1701 SL=100.01 TP=100.45 trail=0.15\n",
            "\n",
            "--- Simulating Tick 20/20 ---\n",
            "Placing BUY order for 1273 of DUMMY with SL=99.37262042329304, TP=99.96153204605572, Trailing=0.19630387425422668\n",
            "ALERT: Placed order ORDER_DUMMY_1273 qty=1273 SL=99.37 TP=99.96 trail=0.20\n",
            "Dynamic live trading simulation complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "new_cell_env"
      },
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "\n",
        "class SLTPEnv(gym.Env):\n",
        "    def __init__(self, price_array, features, regime_ids, max_capital, fee_bps=2):\n",
        "        super().__init__()\n",
        "        self.price = price_array\n",
        "        self.x = features\n",
        "        self.regime = regime_ids\n",
        "        self.max_cap = max_capital\n",
        "        # Discrete grid of (SL, TP, Trail) indices\n",
        "        self.sl_grid = np.array([0.5, 0.75, 1.0])   # in ATR\n",
        "        self.tp_grid = np.array([1.0, 1.5, 2.0])    # in ATR\n",
        "        self.tr_grid = np.array([0.5, 0.75, 1.0])   # in ATR\n",
        "        self.action_map = [(i,j,k) for i in range(3) for j in range(3) for k in range(3)]\n",
        "        self.action_space = gym.spaces.Discrete(len(self.action_map))\n",
        "        obs_dim = self.x.shape[1] + len(np.unique(self.regime))\n",
        "        self.observation_space = gym.spaces.Box(low=-5, high=5, shape=(obs_dim,), dtype=np.float32)\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        super().reset(seed=seed)\n",
        "        self.t = 100  # start after warmup window\n",
        "        self.equity = self.max_cap\n",
        "        self.daily_start_equity = self.equity\n",
        "        self.pos = 0  # 0=flat, 1=long, -1=short\n",
        "        self.entry_price = None\n",
        "        return self._obs(), {}\n",
        "\n",
        "    def _obs(self):\n",
        "        feat = self.x[self.t]\n",
        "        reg = int(self.regime[self.t])\n",
        "        reg_onehot = np.eye(len(np.unique(self.regime)))[reg]\n",
        "        return np.clip(np.concatenate([feat, reg_onehot]), -5, 5).astype(np.float32)\n",
        "\n",
        "    def step(self, action):\n",
        "        i,j,k = self.action_map[action]\n",
        "        sl_atr, tp_atr, tr_atr = self.sl_grid[i], self.tp_grid[j], self.tr_grid[k]\n",
        "\n",
        "        price_t = self.price[self.t]\n",
        "        atr_t   = max(1e-6, self.x[self.t][-1])  # assume last feature is ATR\n",
        "\n",
        "        # Simple position/exit sim (stub): one bar ahead outcome using SL/TP bounds\n",
        "        qty = self._size(sl_atr * atr_t)\n",
        "\n",
        "        pnl = self._simulate_bar(price_t, atr_t, qty, sl_atr, tp_atr, tr_atr)\n",
        "        fee = abs(qty) * price_t * (2/10000)  # 2 bps per side example\n",
        "\n",
        "        self.equity += (pnl - fee)\n",
        "        reward = (pnl - fee) / self.max_cap\n",
        "\n",
        "        done = False\n",
        "        info = {\"equity\": self.equity}\n",
        "\n",
        "        # daily loss breaker (example âˆ’3%)\n",
        "        if (self.equity - self.daily_start_equity) / self.daily_start_equity <= -0.03:\n",
        "            reward -= 0.01\n",
        "            done = True\n",
        "            info[\"breaker\"] = True\n",
        "\n",
        "        self.t += 1\n",
        "        if self.t >= len(self.price)-1:\n",
        "            done = True\n",
        "\n",
        "        return self._obs(), reward, done, False, info\n",
        "\n",
        "    def _size(self, sl_abs):\n",
        "        risk_per_trade = 0.005 * self.equity  # 0.5% per trade\n",
        "        return max(0, int(risk_per_trade / sl_abs))\n",
        "\n",
        "    def _simulate_bar(self, p, atr, qty, sl_atr, tp_atr, tr_atr):\n",
        "        # Placeholder: plug in high/low path and hit-tests vs SL/TP/trailing\n",
        "        # Return signed PnL in currency units\n",
        "        return np.random.randn() * 0.1 * atr * qty"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ecfcf631",
        "outputId": "74d4fe8c-5a08-40dd-fc29-64c84ada9fd7"
      },
      "source": [
        "from stable_baselines3 import DQN\n",
        "# The SLTPEnv and model objects should be available from previous cell executions.\n",
        "# If not, you might need to re-run the environment definition and model loading/training cells.\n",
        "\n",
        "# Load the trained model if it's not already loaded (e.g., if kernel was reset)\n",
        "try:\n",
        "    model = DQN.load(\"dqn_sltp\")\n",
        "    print(\"DQN model loaded successfully for evaluation.\")\n",
        "except Exception as e:\n",
        "    print(f\"Could not load DQN model for evaluation: {e}. Please ensure it has been trained and saved.\")\n",
        "    # Fallback to a dummy model if loading fails\n",
        "    class DummyModel:\n",
        "        def predict(self, observation, deterministic=True):\n",
        "            return np.array([0]), None\n",
        "    model = DummyModel()\n",
        "\n",
        "\n",
        "# Assuming `env` (SLTPEnv instance) is available from previous executions.\n",
        "# If not, create a new instance:\n",
        "# env = SLTPEnv(price_array, features, regime_ids, max_capital=50000)\n",
        "\n",
        "print(\"Starting model evaluation...\")\n",
        "\n",
        "n_eval_episodes = 100\n",
        "episode_rewards = []\n",
        "\n",
        "for episode in range(n_eval_episodes):\n",
        "    obs, info = env.reset()\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "    while not done:\n",
        "        action, _states = model.predict(obs, deterministic=True)\n",
        "        # Fix: Convert action array to scalar integer\n",
        "        action = int(action[0])\n",
        "        obs, reward, done, truncated, info = env.step(action)\n",
        "        total_reward += reward\n",
        "    episode_rewards.append(total_reward)\n",
        "    print(f\"Episode {episode + 1}/{n_eval_episodes} - Total Reward: {total_reward:.4f}\")\n",
        "\n",
        "mean_reward = np.mean(episode_rewards)\n",
        "std_reward = np.std(episode_rewards)\n",
        "\n",
        "print(f\"\\nEvaluation complete over {n_eval_episodes} episodes.\")\n",
        "print(f\"Mean total reward: {mean_reward:.4f}\")\n",
        "print(f\"Standard deviation of total reward: {std_reward:.4f}\")"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Could not load DQN model for evaluation: [Errno 2] No such file or directory: 'dqn_sltp.zip'. Please ensure it has been trained and saved.\n",
            "Starting model evaluation...\n",
            "Episode 1/100 - Total Reward: -0.0400\n",
            "Episode 2/100 - Total Reward: -0.0418\n",
            "Episode 3/100 - Total Reward: -0.0404\n",
            "Episode 4/100 - Total Reward: -0.0409\n",
            "Episode 5/100 - Total Reward: -0.0401\n",
            "Episode 6/100 - Total Reward: -0.0404\n",
            "Episode 7/100 - Total Reward: -0.0401\n",
            "Episode 8/100 - Total Reward: -0.0403\n",
            "Episode 9/100 - Total Reward: -0.0402\n",
            "Episode 10/100 - Total Reward: -0.0412\n",
            "Episode 11/100 - Total Reward: -0.0405\n",
            "Episode 12/100 - Total Reward: -0.0410\n",
            "Episode 13/100 - Total Reward: -0.0409\n",
            "Episode 14/100 - Total Reward: -0.0406\n",
            "Episode 15/100 - Total Reward: -0.0413\n",
            "Episode 16/100 - Total Reward: -0.0405\n",
            "Episode 17/100 - Total Reward: -0.0404\n",
            "Episode 18/100 - Total Reward: -0.0402\n",
            "Episode 19/100 - Total Reward: -0.0400\n",
            "Episode 20/100 - Total Reward: -0.0404\n",
            "Episode 21/100 - Total Reward: -0.0410\n",
            "Episode 22/100 - Total Reward: -0.0402\n",
            "Episode 23/100 - Total Reward: -0.0425\n",
            "Episode 24/100 - Total Reward: -0.0404\n",
            "Episode 25/100 - Total Reward: -0.0409\n",
            "Episode 26/100 - Total Reward: -0.0406\n",
            "Episode 27/100 - Total Reward: -0.0405\n",
            "Episode 28/100 - Total Reward: -0.0412\n",
            "Episode 29/100 - Total Reward: -0.0402\n",
            "Episode 30/100 - Total Reward: -0.0405\n",
            "Episode 31/100 - Total Reward: -0.0404\n",
            "Episode 32/100 - Total Reward: -0.0404\n",
            "Episode 33/100 - Total Reward: -0.0401\n",
            "Episode 34/100 - Total Reward: -0.0402\n",
            "Episode 35/100 - Total Reward: -0.0407\n",
            "Episode 36/100 - Total Reward: -0.0408\n",
            "Episode 37/100 - Total Reward: -0.0402\n",
            "Episode 38/100 - Total Reward: -0.0410\n",
            "Episode 39/100 - Total Reward: -0.0402\n",
            "Episode 40/100 - Total Reward: -0.0412\n",
            "Episode 41/100 - Total Reward: -0.0406\n",
            "Episode 42/100 - Total Reward: -0.0408\n",
            "Episode 43/100 - Total Reward: -0.0410\n",
            "Episode 44/100 - Total Reward: -0.0403\n",
            "Episode 45/100 - Total Reward: -0.0411\n",
            "Episode 46/100 - Total Reward: -0.0413\n",
            "Episode 47/100 - Total Reward: -0.0403\n",
            "Episode 48/100 - Total Reward: -0.0403\n",
            "Episode 49/100 - Total Reward: -0.0413\n",
            "Episode 50/100 - Total Reward: -0.0402\n",
            "Episode 51/100 - Total Reward: -0.0409\n",
            "Episode 52/100 - Total Reward: -0.0405\n",
            "Episode 53/100 - Total Reward: -0.0407\n",
            "Episode 54/100 - Total Reward: -0.0401\n",
            "Episode 55/100 - Total Reward: -0.0411\n",
            "Episode 56/100 - Total Reward: -0.0403\n",
            "Episode 57/100 - Total Reward: -0.0404\n",
            "Episode 58/100 - Total Reward: -0.0405\n",
            "Episode 59/100 - Total Reward: -0.0408\n",
            "Episode 60/100 - Total Reward: -0.0403\n",
            "Episode 61/100 - Total Reward: -0.0401\n",
            "Episode 62/100 - Total Reward: -0.0413\n",
            "Episode 63/100 - Total Reward: -0.0409\n",
            "Episode 64/100 - Total Reward: -0.0402\n",
            "Episode 65/100 - Total Reward: -0.0405\n",
            "Episode 66/100 - Total Reward: -0.0410\n",
            "Episode 67/100 - Total Reward: -0.0401\n",
            "Episode 68/100 - Total Reward: -0.0403\n",
            "Episode 69/100 - Total Reward: -0.0412\n",
            "Episode 70/100 - Total Reward: -0.0404\n",
            "Episode 71/100 - Total Reward: -0.0405\n",
            "Episode 72/100 - Total Reward: -0.0401\n",
            "Episode 73/100 - Total Reward: -0.0404\n",
            "Episode 74/100 - Total Reward: -0.0406\n",
            "Episode 75/100 - Total Reward: -0.0411\n",
            "Episode 76/100 - Total Reward: -0.0407\n",
            "Episode 77/100 - Total Reward: -0.0405\n",
            "Episode 78/100 - Total Reward: -0.0407\n",
            "Episode 79/100 - Total Reward: -0.0403\n",
            "Episode 80/100 - Total Reward: -0.0410\n",
            "Episode 81/100 - Total Reward: -0.0412\n",
            "Episode 82/100 - Total Reward: -0.0407\n",
            "Episode 83/100 - Total Reward: -0.0419\n",
            "Episode 84/100 - Total Reward: -0.0409\n",
            "Episode 85/100 - Total Reward: -0.0412\n",
            "Episode 86/100 - Total Reward: -0.0402\n",
            "Episode 87/100 - Total Reward: -0.0403\n",
            "Episode 88/100 - Total Reward: -0.0407\n",
            "Episode 89/100 - Total Reward: -0.0412\n",
            "Episode 90/100 - Total Reward: -0.0410\n",
            "Episode 91/100 - Total Reward: -0.0405\n",
            "Episode 92/100 - Total Reward: -0.0401\n",
            "Episode 93/100 - Total Reward: -0.0404\n",
            "Episode 94/100 - Total Reward: -0.0418\n",
            "Episode 95/100 - Total Reward: -0.0413\n",
            "Episode 96/100 - Total Reward: -0.0400\n",
            "Episode 97/100 - Total Reward: -0.0401\n",
            "Episode 98/100 - Total Reward: -0.0409\n",
            "Episode 99/100 - Total Reward: -0.0401\n",
            "Episode 100/100 - Total Reward: -0.0401\n",
            "\n",
            "Evaluation complete over 100 episodes.\n",
            "Mean total reward: -0.0406\n",
            "Standard deviation of total reward: 0.0005\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "new_cell_data",
        "outputId": "a423e97d-c282-4313-cb9b-8961559aca10"
      },
      "source": [
        "# Dummy data for training and live trading components\n",
        "import numpy as np\n",
        "\n",
        "# Assuming a sequence length for simulation\n",
        "sequence_length = 2000\n",
        "\n",
        "# Price array (e.g., random walk prices)\n",
        "price_array = 100 + np.cumsum(np.random.randn(sequence_length))\n",
        "\n",
        "# Features (e.g., 5 features, last one being ATR)\n",
        "# Make sure ATR is non-negative and somewhat realistic\n",
        "features = np.random.randn(sequence_length, 5)\n",
        "features[:, -1] = np.abs(features[:, -1]) * 0.5 + 0.1 # Ensure ATR is positive\n",
        "\n",
        "# Regime IDs (e.g., 3 different regimes)\n",
        "regime_ids = np.random.randint(0, 3, sequence_length)\n",
        "\n",
        "print(\"Dummy data created for price_array, features, and regime_ids.\")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dummy data created for price_array, features, and regime_ids.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "new_cell_alerts",
        "outputId": "7a07241d-9e3d-4be0-8f53-ea1b9ccf0930"
      },
      "source": [
        "# alerts.py\n",
        "def send_alert(message):\n",
        "    print(f\"ALERT: {message}\")\n",
        "\n",
        "print(\"Alerts module loaded.\")"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Alerts module loaded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "new_cell_broker",
        "outputId": "7e5fc866-c9ac-46c9-decd-396034839878"
      },
      "source": [
        "# broker.py\n",
        "class Broker:\n",
        "    def __init__(self, paper=True):\n",
        "        print(f\"Broker initialized (Paper Trading: {paper})\")\n",
        "        self._equity = 50000.0  # Placeholder initial equity\n",
        "\n",
        "    def account_equity(self):\n",
        "        # Placeholder: returns current equity\n",
        "        return self._equity\n",
        "\n",
        "    def place_bracket_order(self, symbol, side, qty, sl_price, tp_price, trailing):\n",
        "        # Placeholder: simulates placing an order\n",
        "        print(f\"Placing {side} order for {qty} of {symbol} with SL={sl_price}, TP={tp_price}, Trailing={trailing}\")\n",
        "        # In a real scenario, this would interact with a broker API\n",
        "        return f\"ORDER_{symbol}_{qty}\" # Returns a dummy order ID\n",
        "\n",
        "    def flatten_all(self):\n",
        "        # Placeholder: simulates flattening all positions\n",
        "        print(\"All positions flattened.\")\n",
        "\n",
        "    def request_withdrawal(self, amount):\n",
        "        # Placeholder: simulates a withdrawal request\n",
        "        print(f\"Requesting withdrawal of {amount}.\")\n",
        "        return True # Always succeeds in this placeholder\n",
        "\n",
        "print(\"Broker module loaded.\")"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Broker module loaded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "new_cell_live_trader",
        "outputId": "6fcbcdc9-de0e-4584-a397-0eee07ef7505"
      },
      "source": [
        "from stable_baselines3 import DQN\n",
        "import numpy as np\n",
        "\n",
        "# The following modules are defined in separate cells and are globally available.\n",
        "# No need for explicit 'from ... import ...' statements for them in a notebook environment.\n",
        "# from alerts import send_alert\n",
        "# from broker import Broker\n",
        "# from risk import RiskManager\n",
        "\n",
        "MAX_CAPITAL = 50000\n",
        "DAILY_LOSS_LIMIT = 0.03  # 3%\n",
        "\n",
        "broker = Broker(paper=True) # Broker class is available globally\n",
        "risk = RiskManager(MAX_CAPITAL, DAILY_LOSS_LIMIT) # RiskManager class is available globally\n",
        "# The model will be loaded after training, so this line might fail if not trained yet.\n",
        "# For now, let's assume it's available or handle the error gracefully for placeholders.\n",
        "try:\n",
        "    model = DQN.load(\"dqn_sltp\")\n",
        "except Exception as e:\n",
        "    print(f\"Could not load DQN model: {e}. Please ensure it has been trained and saved.\")\n",
        "    # Create a dummy model for execution flow if load fails\n",
        "    class DummyModel:\n",
        "        def predict(self, observation, deterministic=True):\n",
        "            # Returns a dummy action (e.g., action 0)\n",
        "            # Ensure the observation is correctly shaped (1, N) for model.predict\n",
        "            return np.array([0]), None\n",
        "    model = DummyModel()\n",
        "\n",
        "# Assuming SLTPEnv's x.shape[1] (features) is 5 and number of unique regimes is 3\n",
        "num_features_from_x = 5\n",
        "num_regimes = 3\n",
        "\n",
        "class LiveTraderState:\n",
        "    def __init__(self, observation_for_model, atr_value):\n",
        "        self.observation = observation_for_model\n",
        "        self.atr = atr_value\n",
        "\n",
        "class DummyTick:\n",
        "    def __init__(self):\n",
        "        self.price = 100.0\n",
        "        self.symbol = \"DUMMY\"\n",
        "        # Generate dummy features for the tick, matching expected input for build_state\n",
        "        self.features = np.random.randn(num_features_from_x).astype(np.float32)\n",
        "        # Ensure ATR (last feature) is positive\n",
        "        self.features[-1] = np.abs(self.features[-1]) * 0.5 + 0.1 # Ensure ATR is positive\n",
        "        self.regime_id = np.random.randint(0, num_regimes)\n",
        "\n",
        "def build_state(tick):\n",
        "    # Use DummyTick if the provided tick is not sufficient\n",
        "    if not isinstance(tick, DummyTick) and (not hasattr(tick, 'features') or not hasattr(tick, 'regime_id')):\n",
        "        tick = DummyTick()\n",
        "\n",
        "    feat = tick.features\n",
        "    reg = int(tick.regime_id)\n",
        "    reg_onehot = np.eye(num_regimes)[reg]\n",
        "\n",
        "    # The observation for the model needs to be a flat array\n",
        "    obs_for_model = np.concatenate([feat, reg_onehot]).astype(np.float32)\n",
        "\n",
        "    return LiveTraderState(obs_for_model, feat[-1]) # feat[-1] is assumed to be ATR\n",
        "\n",
        "def decode_action(action):\n",
        "    sl_grid = np.array([0.5, 0.75, 1.0])   # in ATR\n",
        "    tp_grid = np.array([1.0, 1.5, 2.0])    # in ATR\n",
        "    tr_grid = np.array([0.5, 0.75, 1.0])   # in ATR\n",
        "\n",
        "    # Decode action from a single integer back to (i, j, k) indices\n",
        "    k = action % 3\n",
        "    j = (action // 3) % 3\n",
        "    i = (action // 9) % 3\n",
        "\n",
        "    return sl_grid[i], tp_grid[j], tr_grid[k]\n",
        "\n",
        "def try_request_withdrawal():\n",
        "    ok = broker.request_withdrawal(amount=risk.sweep_amount())\n",
        "    send_alert(\"Withdrawal request \" + (\"submitted.\" if ok else \"FAILED. Manual action needed.\")) # send_alert is available globally\n",
        "\n",
        "def on_market_tick(tick):\n",
        "    live_state = build_state(tick)\n",
        "\n",
        "    # model.predict expects a 2D array: (num_samples, num_features)\n",
        "    # Reshape live_state.observation from (N,) to (1, N)\n",
        "    # Fix: Convert action array to scalar integer to avoid DeprecationWarning\n",
        "    action = int(model.predict(live_state.observation.reshape(1, -1), deterministic=True)[0])\n",
        "    sl, tp, tr = decode_action(action)\n",
        "\n",
        "    if not risk.can_trade(broker.account_equity()):\n",
        "        send_alert(\"Breaker active: trading halted.\")\n",
        "        return\n",
        "\n",
        "    # Ensure tick object has a 'price' attribute\n",
        "    current_price = tick.price if hasattr(tick, 'price') else 100.0 # Use dummy price if not available\n",
        "\n",
        "    qty, sl_price, tp_price, trail = risk.compute_order_params(current_price, sl, tp, tr, live_state.atr)\n",
        "\n",
        "    if qty > 0:\n",
        "        order_id = broker.place_bracket_order(\n",
        "            symbol=tick.symbol if hasattr(tick, 'symbol') else \"DUMMY_SYMBOL\",\n",
        "            side=\"BUY\",\n",
        "            qty=qty,\n",
        "            sl_price=sl_price,\n",
        "            tp_price=tp_price,\n",
        "            trailing=trail\n",
        "        )\n",
        "        send_alert(f\"Placed order {order_id} qty={qty} SL={sl_price:.2f} TP={tp_price:.2f} trail={trail:.2f}\")\n",
        "\n",
        "    if risk.hit_hard_stop(broker.account_equity()):\n",
        "        broker.flatten_all()\n",
        "        send_alert(\"HARD STOP hit. Flattened. Initiating withdrawal request.\")\n",
        "        try_request_withdrawal()\n"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Broker initialized (Paper Trading: True)\n",
            "Could not load DQN model: [Errno 2] No such file or directory: 'dqn_sltp.zip'. Please ensure it has been trained and saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "new_cell_risk"
      },
      "source": [
        "class RiskManager:\n",
        "    def __init__(self, max_capital, daily_loss_limit):\n",
        "        self.max_capital = max_capital\n",
        "        self.daily_start = None\n",
        "        self.daily_loss_limit = daily_loss_limit\n",
        "\n",
        "    def start_day(self, equity):\n",
        "        self.daily_start = equity\n",
        "\n",
        "    def can_trade(self, equity):\n",
        "        if self.daily_start is None: self.start_day(equity)\n",
        "        dd = (equity - self.daily_start) / self.daily_start\n",
        "        return dd > -self.daily_loss_limit and equity <= self.max_capital\n",
        "\n",
        "    def compute_order_params(self, price, sl_atr, tp_atr, tr_atr, atr):\n",
        "        sl_abs = sl_atr * atr\n",
        "        tp_abs = tp_atr * atr\n",
        "        trail_abs = tr_atr * atr\n",
        "        risk_per_trade = 0.005 * self.max_capital\n",
        "        qty = max(0, int(risk_per_trade / sl_abs))\n",
        "        return qty, price - sl_abs, price + tp_abs, trail_abs\n",
        "\n",
        "    def hit_hard_stop(self, equity):\n",
        "        dd = (equity - self.daily_start) / self.daily_start\n",
        "        return dd <= -self.daily_loss_limit\n",
        "\n",
        "    def sweep_amount(self):\n",
        "        # example: sweep anything above a safety buffer\n",
        "        return max(0, self.max_capital * 0.2)\n"
      ],
      "execution_count": 16,
      "outputs": []
    }
  ]
}