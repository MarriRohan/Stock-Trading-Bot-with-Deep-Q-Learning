{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNxvDAJYy9hklI4zYS6MgJV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MarriRohan/Stock-Trading-Bot-with-Deep-Q-Learning/blob/main/Stock-Trading-Bot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "bG62UJSE2EeK"
      },
      "outputs": [],
      "source": [
        "\n",
        "!pip install -q torch PyYAML\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import math\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import yaml\n",
        "from typing import Tuple, Dict, Any\n",
        "\n",
        "# Example config (you can also load from YAML file)\n",
        "CFG = {\n",
        "    \"max_capital\": 100000,\n",
        "    \"daily_loss_limit\": 0.02,    # 2% of capital\n",
        "    \"drawdown_limit\": 0.15,      # 15% peak-to-trough\n",
        "    \"risk_per_trade\": 0.01,      # 1% risk per trade\n",
        "    \"sl_grid\": [0.5, 0.75, 1.0], # multiples of ATR (we'll use price range proxy)\n",
        "    \"tp_grid\": [1.0, 1.5, 2.0],\n",
        "    \"trailing_grid\": [0.5, 0.75, 1.0],\n",
        "    \"learning_rate\": 5e-4,\n",
        "    \"gamma\": 0.99,\n",
        "    \"batch_size\": 64,\n",
        "    \"replay_size\": 20000,\n",
        "    \"train_start\": 500,\n",
        "    \"episodes\": 20,\n",
        "    \"sync_every\": 5\n",
        "}\n"
      ],
      "metadata": {
        "id": "znIMGQKT8Q_h"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build discrete action grid that maps an integer action index\n",
        "# to (sl_mult, tp_mult, trailing_mult) where multipliers are relative to ATR proxy.\n",
        "def build_action_grid(sl_grid, tp_grid, trailing_grid):\n",
        "    grid = []\n",
        "    for sl in sl_grid:\n",
        "        for tp in tp_grid:\n",
        "            for tr in trailing_grid:\n",
        "                grid.append((sl, tp, tr))\n",
        "    return grid\n",
        "\n",
        "ACTION_GRID = build_action_grid(CFG[\"sl_grid\"], CFG[\"tp_grid\"], CFG[\"trailing_grid\"])\n",
        "ACTION_DIM = len(ACTION_GRID)\n",
        "print(\"Action grid size:\", ACTION_DIM)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QAwme9JX8ZGb",
        "outputId": "4cab8db0-064a-467f-e8a0-aee624f9880d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Action grid size: 27\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TradingEnv:\n",
        "    \"\"\"\n",
        "    Minimal trading environment for demo.\n",
        "    - data: list/array of prices (close)\n",
        "    - action: integer index mapping to (SL_mult, TP_mult, trailing_mult)\n",
        "    - position sizing uses risk_per_trade and SL distance to compute size\n",
        "    - Enforces MAX_CAPITAL, DAILY_LOSS_LIMIT, DRAWDOWN_LIMIT\n",
        "    NOTE: This is a simplified env for demonstration and testing only.\n",
        "    \"\"\"\n",
        "    def __init__(self, prices, config: Dict[str, Any]):\n",
        "        self.prices = np.asarray(prices, dtype=float)\n",
        "        self.cfg = config\n",
        "        self.max_capital = config[\"max_capital\"]\n",
        "        self.daily_loss_limit = config[\"daily_loss_limit\"]\n",
        "        self.drawdown_limit = config[\"drawdown_limit\"]\n",
        "        self.risk_per_trade = config[\"risk_per_trade\"]\n",
        "        self.action_grid = ACTION_GRID\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.wallet = float(self.max_capital)\n",
        "        self.equity_peak = float(self.wallet)\n",
        "        self.position = None           # dict: {'size', 'entry_price', 'sl', 'tp', 'trailing'}\n",
        "        self.current_step = 0\n",
        "        self.daily_start_equity = float(self.wallet)\n",
        "        self.done = False\n",
        "        return self._get_obs()\n",
        "\n",
        "    def step(self, action_idx: int) -> Tuple[np.ndarray, float, bool, dict]:\n",
        "        info = {}\n",
        "        price = float(self.prices[self.current_step])\n",
        "        reward = 0.0\n",
        "\n",
        "        # Map action index to sl/tp/tr multipliers\n",
        "        sl_mult, tp_mult, tr_mult = self.action_grid[action_idx]\n",
        "\n",
        "        # Use a simple ATR proxy: local range of last 5 bars (if available) else 1.0\n",
        "        window = 5\n",
        "        if self.current_step >= 1:\n",
        "            start = max(0, self.current_step - window)\n",
        "            local = self.prices[start:self.current_step+1]\n",
        "            atr_proxy = float(np.mean(np.abs(np.diff(local)))) if len(local) > 1 else 1.0\n",
        "            if atr_proxy <= 0:\n",
        "                atr_proxy = 1.0\n",
        "        else:\n",
        "            atr_proxy = 1.0\n",
        "\n",
        "        # If flat, open a long position of size computed by risk per trade and SL distance\n",
        "        if self.position is None:\n",
        "            # Suppose we open LONG only for demo\n",
        "            sl_distance = sl_mult * atr_proxy\n",
        "            if sl_distance <= 0:\n",
        "                sl_distance = 1.0\n",
        "            # Risk amount in absolute rupees\n",
        "            risk_amount = self.risk_per_trade * self.wallet\n",
        "            # position size (number of units) = risk / sl_distance\n",
        "            size = risk_amount / sl_distance\n",
        "            # compute SL and TP prices\n",
        "            sl_price = price - sl_distance\n",
        "            tp_price = price + tp_mult * atr_proxy\n",
        "            # trailing amount in price units\n",
        "            trailing_amt = tr_mult * atr_proxy\n",
        "            self.position = {\n",
        "                \"size\": size,\n",
        "                \"entry_price\": price,\n",
        "                \"sl\": sl_price,\n",
        "                \"tp\": tp_price,\n",
        "                \"trailing_amt\": trailing_amt,\n",
        "                \"peak_price\": price  # used for trailing stop logic\n",
        "            }\n",
        "            info[\"order\"] = \"open_long\"\n",
        "        else:\n",
        "            # Evaluate the position at current price and check SL/TP/trailing\n",
        "            p = price\n",
        "            pos = self.position\n",
        "            size = pos[\"size\"]\n",
        "            entry = pos[\"entry_price\"]\n",
        "\n",
        "            # update peak price for trailing\n",
        "            if p > pos[\"peak_price\"]:\n",
        "                pos[\"peak_price\"] = p\n",
        "\n",
        "            # Check TP\n",
        "            if p >= pos[\"tp\"]:\n",
        "                pnl = (pos[\"tp\"] - entry) * size\n",
        "                self.wallet += pnl\n",
        "                reward = pnl\n",
        "                info[\"exit\"] = \"tp_hit\"\n",
        "                self.position = None\n",
        "\n",
        "            # Check SL\n",
        "            elif p <= pos[\"sl\"]:\n",
        "                pnl = (pos[\"sl\"] - entry) * size\n",
        "                self.wallet += pnl\n",
        "                reward = pnl\n",
        "                info[\"exit\"] = \"sl_hit\"\n",
        "                self.position = None\n",
        "\n",
        "            else:\n",
        "                # Check trailing stop: if price has fallen from peak more than trailing_amt\n",
        "                if pos[\"peak_price\"] - p >= pos[\"trailing_amt\"]:\n",
        "                    # execute at current price\n",
        "                    pnl = (p - entry) * size\n",
        "                    self.wallet += pnl\n",
        "                    reward = pnl\n",
        "                    info[\"exit\"] = \"trailing_hit\"\n",
        "                    self.position = None\n",
        "                else:\n",
        "                    # hold â€” no reward this tick\n",
        "                    reward = 0.0\n",
        "\n",
        "        # Update equity peak/drawdown & daily loss\n",
        "        if self.wallet > self.equity_peak:\n",
        "            self.equity_peak = self.wallet\n",
        "        drawdown = (self.equity_peak - self.wallet) / max(1.0, self.equity_peak)\n",
        "        if drawdown >= self.drawdown_limit:\n",
        "            info[\"circuit\"] = \"drawdown_limit\"\n",
        "            self.done = True\n",
        "\n",
        "        daily_loss = (self.daily_start_equity - self.wallet) / max(1.0, self.daily_start_equity)\n",
        "        if daily_loss >= self.daily_loss_limit:\n",
        "            info[\"circuit\"] = \"daily_loss_limit\"\n",
        "            self.done = True\n",
        "\n",
        "        # Move next\n",
        "        self.current_step += 1\n",
        "        if self.current_step >= len(self.prices) - 1:\n",
        "            # close any open position at last price\n",
        "            if self.position is not None:\n",
        "                final_price = float(self.prices[-1])\n",
        "                pnl = (final_price - self.position[\"entry_price\"]) * self.position[\"size\"]\n",
        "                self.wallet += pnl\n",
        "                reward += pnl\n",
        "                info[\"exit\"] = info.get(\"exit\", \"\") + \"|final_close\"\n",
        "                self.position = None\n",
        "            self.done = True\n",
        "\n",
        "        obs = self._get_obs()\n",
        "        return obs, float(reward), bool(self.done), info\n",
        "\n",
        "    def _get_obs(self) -> np.ndarray:\n",
        "        # Observation: [price, wallet_ratio, equity_peak_ratio, step_norm]\n",
        "        price = float(self.prices[self.current_step])\n",
        "        wallet_ratio = self.wallet / max(1.0, self.max_capital)\n",
        "        peak_ratio = self.equity_peak / max(1.0, self.max_capital)\n",
        "        step_norm = float(self.current_step) / max(1, len(self.prices))\n",
        "        return np.array([price, wallet_ratio, peak_ratio, step_norm], dtype=np.float32)\n"
      ],
      "metadata": {
        "id": "1yDqu_Be8bcJ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleQNet(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "class DQNAgent:\n",
        "    def __init__(self, state_dim, action_dim, cfg):\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        self.q = SimpleQNet(state_dim, action_dim)\n",
        "        self.target_q = SimpleQNet(state_dim, action_dim)\n",
        "        self.optimizer = optim.Adam(self.q.parameters(), lr=cfg.get(\"learning_rate\", 1e-3))\n",
        "        self.replay = deque(maxlen=cfg.get(\"replay_size\", 50000))\n",
        "        self.batch_size = cfg.get(\"batch_size\", 64)\n",
        "        self.gamma = cfg.get(\"gamma\", 0.99)\n",
        "        self.epsilon = 1.0\n",
        "        self.eps_min = 0.05\n",
        "        self.eps_decay = 0.995\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.q.to(self.device)\n",
        "        self.target_q.to(self.device)\n",
        "\n",
        "    def act(self, state: np.ndarray) -> int:\n",
        "        if random.random() < self.epsilon:\n",
        "            return random.randrange(self.action_dim)\n",
        "        self.q.eval()\n",
        "        with torch.no_grad():\n",
        "            s_t = torch.tensor(state, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
        "            qvals = self.q(s_t)\n",
        "            action = int(torch.argmax(qvals, dim=1).cpu().item())\n",
        "        self.q.train()\n",
        "        return action\n",
        "\n",
        "    def remember(self, s, a, r, s2, done):\n",
        "        self.replay.append((s, a, r, s2, done))\n",
        "\n",
        "    def learn(self):\n",
        "        if len(self.replay) < self.batch_size:\n",
        "            return\n",
        "        batch = random.sample(self.replay, self.batch_size)\n",
        "        s, a, r, s2, done = zip(*batch)\n",
        "        s = torch.tensor(np.stack(s), dtype=torch.float32, device=self.device)\n",
        "        a = torch.tensor(a, dtype=torch.int64, device=self.device).unsqueeze(1)\n",
        "        r = torch.tensor(r, dtype=torch.float32, device=self.device).unsqueeze(1)\n",
        "        s2 = torch.tensor(np.stack(s2), dtype=torch.float32, device=self.device)\n",
        "        done = torch.tensor(done, dtype=torch.float32, device=self.device).unsqueeze(1)\n",
        "\n",
        "        qvals = self.q(s).gather(1, a)\n",
        "        with torch.no_grad():\n",
        "            qnext = self.target_q(s2).max(1)[0].unsqueeze(1)\n",
        "        target = r + (1.0 - done) * self.gamma * qnext\n",
        "        loss = nn.functional.mse_loss(qvals, target)\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        # epsilon decay\n",
        "        if self.epsilon > self.eps_min:\n",
        "            self.epsilon *= self.eps_decay\n",
        "\n",
        "    def sync_target(self):\n",
        "        self.target_q.load_state_dict(self.q.state_dict())\n"
      ],
      "metadata": {
        "id": "HDUb_6ce8fbP"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Toy price series: sine wave + linear drift to simulate market moves\n",
        "def make_toy_prices(n=1000):\n",
        "    x = np.linspace(0, 20 * math.pi, n)\n",
        "    prices = (np.sin(x) * 5.0) + (np.linspace(0, 20, n)) + 100.0\n",
        "    # add small noise\n",
        "    prices += np.random.normal(scale=0.5, size=n)\n",
        "    return prices\n",
        "\n",
        "prices = make_toy_prices(800)\n",
        "env = TradingEnv(prices, CFG)\n",
        "state_dim = 4\n",
        "action_dim = ACTION_DIM\n",
        "agent = DQNAgent(state_dim, action_dim, CFG)\n",
        "\n",
        "# Training loop (fast demo)\n",
        "for ep in range(CFG[\"episodes\"]):\n",
        "    s = env.reset()\n",
        "    done = False\n",
        "    total_reward = 0.0\n",
        "    steps = 0\n",
        "    while not done:\n",
        "        a = agent.act(s)\n",
        "        s2, r, done, info = env.step(a)\n",
        "        agent.remember(s, a, r, s2, done)\n",
        "        agent.learn()\n",
        "        s = s2\n",
        "        total_reward += r\n",
        "        steps += 1\n",
        "    if ep % CFG[\"sync_every\"] == 0:\n",
        "        agent.sync_target()\n",
        "    print(f\"Episode {ep+1}/{CFG['episodes']}  steps={steps}  total_reward={total_reward:.2f}  final_wallet={env.wallet:.2f}  info={info}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "la0ofcu48h_4",
        "outputId": "9e719ac5-e076-41be-fff7-2b7332a6e674"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1/20  steps=59  total_reward=-2631.80  final_wallet=97368.20  info={'exit': 'sl_hit', 'circuit': 'daily_loss_limit'}\n",
            "Episode 2/20  steps=799  total_reward=109974.50  final_wallet=209974.50  info={'exit': '|final_close'}\n",
            "Episode 3/20  steps=783  total_reward=251179.31  final_wallet=351179.31  info={'exit': 'sl_hit', 'circuit': 'drawdown_limit'}\n",
            "Episode 4/20  steps=55  total_reward=-2009.49  final_wallet=97990.51  info={'exit': 'sl_hit', 'circuit': 'daily_loss_limit'}\n",
            "Episode 5/20  steps=691  total_reward=189697.28  final_wallet=289697.28  info={'exit': 'sl_hit', 'circuit': 'drawdown_limit'}\n",
            "Episode 6/20  steps=783  total_reward=176767.00  final_wallet=276767.00  info={'exit': 'sl_hit', 'circuit': 'drawdown_limit'}\n",
            "Episode 7/20  steps=783  total_reward=202155.57  final_wallet=302155.57  info={'exit': 'sl_hit', 'circuit': 'drawdown_limit'}\n",
            "Episode 8/20  steps=761  total_reward=157033.49  final_wallet=257033.49  info={'exit': 'sl_hit', 'circuit': 'drawdown_limit'}\n",
            "Episode 9/20  steps=51  total_reward=-2896.44  final_wallet=97103.56  info={'exit': 'sl_hit', 'circuit': 'daily_loss_limit'}\n",
            "Episode 10/20  steps=783  total_reward=265265.36  final_wallet=365265.36  info={'exit': 'sl_hit', 'circuit': 'drawdown_limit'}\n",
            "Episode 11/20  steps=687  total_reward=198300.64  final_wallet=298300.64  info={'exit': 'sl_hit', 'circuit': 'drawdown_limit'}\n",
            "Episode 12/20  steps=799  total_reward=238446.37  final_wallet=338446.37  info={'exit': '|final_close'}\n",
            "Episode 13/20  steps=619  total_reward=188754.26  final_wallet=288754.26  info={'exit': 'sl_hit', 'circuit': 'drawdown_limit'}\n",
            "Episode 14/20  steps=783  total_reward=243981.76  final_wallet=343981.76  info={'exit': 'sl_hit', 'circuit': 'drawdown_limit'}\n",
            "Episode 15/20  steps=799  total_reward=180820.27  final_wallet=280820.27  info={'exit': '|final_close'}\n",
            "Episode 16/20  steps=783  total_reward=168936.44  final_wallet=268936.44  info={'exit': 'sl_hit', 'circuit': 'drawdown_limit'}\n",
            "Episode 17/20  steps=799  total_reward=152832.65  final_wallet=252832.65  info={'exit': '|final_close'}\n",
            "Episode 18/20  steps=783  total_reward=197837.27  final_wallet=297837.27  info={'exit': 'sl_hit', 'circuit': 'drawdown_limit'}\n",
            "Episode 19/20  steps=783  total_reward=212592.87  final_wallet=312592.87  info={'exit': 'sl_hit', 'circuit': 'drawdown_limit'}\n",
            "Episode 20/20  steps=783  total_reward=298468.12  final_wallet=398468.12  info={'exit': 'sl_hit', 'circuit': 'drawdown_limit'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save Q-network weights (optional in Colab)\n",
        "torch.save(agent.q.state_dict(), \"dqn_qnet.pth\")\n",
        "print(\"Model saved to dqn_qnet.pth\")\n",
        "\n",
        "# Quick one-episode evaluation with greedy policy\n",
        "env_eval = TradingEnv(prices, CFG)\n",
        "s = env_eval.reset()\n",
        "done = False\n",
        "total = 0.0\n",
        "while not done:\n",
        "    # greedy action\n",
        "    agent.epsilon = 0.0\n",
        "    a = agent.act(s)\n",
        "    s, r, done, info = env_eval.step(a)\n",
        "    total += r\n",
        "print(\"Eval final wallet:\", env_eval.wallet, \"total pnl:\", total, \"info:\", info)\n"
      ],
      "metadata": {
        "id": "JG2DHUHq8j6W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "528f941d-1cb4-4a01-c8da-20d1679f97e7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved to dqn_qnet.pth\n",
            "Eval final wallet: 365828.13565495634 total pnl: 265828.1356549564 info: {'exit': 'sl_hit', 'circuit': 'drawdown_limit'}\n"
          ]
        }
      ]
    }
  ]
}